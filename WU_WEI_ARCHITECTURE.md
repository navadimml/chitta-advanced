# Chitta's Wu Wei Architecture: Conversation-First, Dependency-Based Design

**Document Version**: 3.5
**Date**: November 18, 2025
**Status**: Implemented & Simplified (×¤×©×•×˜ - × ×˜×•×œ ×—×œ×§×™× ×¢×•×“×¤×™×)

**Latest Update**: Wu Wei v3.5 - Event-Driven Card Architecture + Domain-Agnostic Context

---

## Version History

- **v1.0** (Nov 9, 2025): Phase-based workflow (`phases.yaml`) - Traditional stage gates
- **v2.0** (Nov 9, 2025): Wu Wei dependency graph - Separate `artifacts`, `capabilities`, `lifecycle_events` sections
- **v3.0** (Nov 11, 2025): **Simplified Wu Wei** - Unified `moments` structure (50% less configuration, 100% functionality)
- **v3.5** (Nov 18, 2025): **Event-Driven Cards** - Cards unified with moments, domain-agnostic context, accurate message timing

### What's New in v3.0

**Before (v2.0)** - Three redundant sections:
```yaml
artifacts:
  baseline_video_guidelines:
    prerequisites: { knowledge_is_rich: true }
    unlocks: [video_upload]
    event: guidelines_ready  # Links to separate section

lifecycle_events:
  guidelines_ready:
    message: "×”×”× ×—×™×•×ª ××•×›× ×•×ª!"
    ui_context: {...}

capabilities:
  video_upload:
    prerequisites: { ... }  # DUPLICATE!
```

**After (v3.0)** - One unified section:
```yaml
moments:
  guidelines_ready:
    when: { knowledge_is_rich: true }
    artifact: "baseline_video_guidelines"
    message: "×”×”× ×—×™×•×ª ××•×›× ×•×ª!"
    ui: { type: "card", default: "..." }
    unlocks: ["upload_videos"]
```

**Result**: 208 lines (down from 360), zero redundancy, same functionality.

### What's New in v3.5

**Event-Driven Card Architecture** - Cards now unified with moments:

**Before (v3.0)** - Cards duplicated prerequisites:
```yaml
# lifecycle_events.yaml
moments:
  guidelines_ready:
    when: { knowledge_is_rich: true }
    artifact: "baseline_video_guidelines"
    unlocks: ["view_video_guidelines"]

# context_cards.yaml (separate file)
cards:
  guidelines_ready_card:
    display_conditions:  # âŒ DUPLICATE prerequisites
      artifacts.baseline_video_guidelines.exists: true
      user_actions.viewed_guidelines: false
    content:
      title: "×”×”× ×—×™×•×ª ××•×›× ×•×ª! ğŸ¬"
```

**After (v3.5)** - Cards live IN moments:
```yaml
moments:
  guidelines_ready:
    when: { knowledge_is_rich: true }
    artifact: "baseline_video_guidelines"

    # ğŸŒŸ Card displays when moment triggers (event-driven)
    card:
      card_type: success
      title: "×”×”× ×—×™×•×ª ××•×›× ×•×ª! ğŸ¬"
      body: "×”×›× ×ª×™ ×œ×š ×”× ×—×™×•×ª..."
      actions: [view_video_guidelines]
```

**Domain-Agnostic Context** - Backend no longer hardcodes fields:
```python
# Before: Domain-specific field extraction
context = {
    "child_name": extracted_data.get("child_name"),  # âŒ Hardcoded
    "primary_concerns": extracted_data.get("primary_concerns"),
}

# After: Generic structure passing
context = {
    "extracted_data": session.extracted_data,  # âœ… Cards pick what they need
    "artifacts": session.artifacts,
}
```

**Accurate Message Timing** - User messages saved BEFORE lifecycle checks:
- Fixes off-by-one message_count bug
- Ensures knowledge_is_rich triggers at correct time
- Moments appear when they should

**Result**: Zero card duplication, domain-agnostic backend, accurate prerequisites.

---

## Table of Contents

1. [Philosophy: The Essence of Wu Wei](#philosophy-the-essence-of-wu-wei)
2. [The Problem with Stage-Based Thinking](#the-problem-with-stage-based-thinking)
3. [The Solution: Continuous Conversation + Dependency Graph](#the-solution-continuous-conversation--dependency-graph)
4. [Core Principles](#core-principles)
5. [Technical Architecture](#technical-architecture)
6. [User Experience Journey](#user-experience-journey)
7. [Implementation Guide](#implementation-guide)
8. [Examples & Scenarios](#examples--scenarios)

---

## Philosophy: The Essence of Wu Wei

### What is Wu Wei?

**Wu Wei** (ç„¡ç‚º) means "effortless action" or "action through non-action" in Taoist philosophy. It's the art of accomplishing goals by working with natural forces rather than against them.

In Chitta's context, Wu Wei means:

- **Natural flow** over forced progression
- **Parent agency** over system control
- **Gentle guidance** over rigid gates
- **Emerging capabilities** over locked stages
- **Conversation primacy** over workflow steps

### Wu Wei Applied to Chitta

Traditional software forces users through rigid workflows:
```
Step 1 â†’ Complete Step 1 â†’ Unlock Step 2 â†’ Complete Step 2 â†’ Unlock Step 3
```

Chitta flows naturally like conversation:
```
Conversation (ongoing)
    â†“
Knowledge accumulates
    â†“
Capabilities emerge when ready
    â†“
Parent explores freely
    â†“
Chitta guides gently when needed
```

**Key insight**: Parents don't complete "stages" - they have **conversations** that naturally accumulate **knowledge**, which gradually **unlocks capabilities**.

---

## The Problem with Stage-Based Thinking

### What We Were Doing Wrong

**Stage-based architecture** (the old way):

```yaml
stages:
  - interview:
      required: true
      locked_until: "start"
      blocks: [guidelines, upload, analysis]

  - guidelines_preparation:
      required: true
      locked_until: "interview complete"
      blocks: [upload, analysis]

  - video_upload:
      required: true
      locked_until: "guidelines viewed"
      blocks: [analysis]

  - analysis:
      locked_until: "videos uploaded"
```

**Problems with this approach:**

1. **âŒ False boundaries**: There's no clear "interview ending" - conversation is continuous
2. **âŒ Parent feels constrained**: Can't explore or ask "what's next?" during "interview"
3. **âŒ Rigid progression**: Must complete A before B, even if parent wants to skip
4. **âŒ Chitta can't adapt**: Locked into stage-specific behaviors
5. **âŒ Artificial gates**: "You can't do X until Y is complete" feels restrictive
6. **âŒ Poor UX**: Progress bars like "76%" feel incomplete when parent feels done

### The "Interview" Misconception

We were thinking:
> "Interview is a phase that must be completed before moving to the next phase"

**Reality**:
> "Interview is just the **first conversation**. Conversation never stops. Knowledge accumulates continuously. The parent can ask about filming or uploading **at any time**, and Chitta responds based on **what's available**, not what **stage** we're in."

---

## The Solution: Continuous Conversation + Dependency Graph

### The New Mental Model

**Three core concepts:**

1. **Conversation**: Always ongoing, never "complete"
2. **Knowledge**: Accumulates continuously from conversation
3. **Capabilities**: Unlock when prerequisites (knowledge) are met

```
                    CONVERSATION (continuous)
                           â†“
                    KNOWLEDGE GRAPH
                (what we know about family)
                           â†“
                  PREREQUISITE CHECKING
                (do we have enough to do X?)
                           â†“
                   CAPABILITIES UNLOCK
              (cards appear, actions available)
                           â†“
                    PARENT EXPLORES
                  (uses capabilities freely)
                           â†“
              More conversation, more knowledge...
```

### No Phases, Only Dependencies

Instead of:
```python
if phase == "interview":
    allow_conversation()
    block_guidelines()
    block_upload()
```

We have:
```python
# Check what's possible based on current knowledge
capabilities = {
    "generate_guidelines": check_prerequisites("video_guidelines", knowledge),
    "upload_video": check_prerequisites("video_upload", knowledge),
    "view_report": check_prerequisites("assessment_report", knowledge),
    "ask_questions": True,  # ALWAYS available
}

# Show what's available, hide what's not
# Parent can ask about anything, Chitta guides based on readiness
```

---

## Core Principles

### 1. Conversation Never Stops

**Principle**: Conversation is the **primary mode** of interaction, not a phase.

**Implications**:
- Breadcrumb always shows: "××©×•×—×—×™× ×¢× ×¦'×™×˜×” ğŸ’¬"
- Input area is always available (except during backend processing)
- Parent can ask "××” ×”×œ××”?" or "××™×š ×–×” ×¢×•×‘×“?" **at any point**
- Other activities (uploading, viewing reports) are **contextual additions**, not replacements

**Example**:
```
Parent is viewing video guidelines deep view
Input area still visible at bottom: "×™×© ×œ×š ×©××œ×•×ª?"
Parent can ask: "×œ××” ×¦×¨×™×š ×œ×¦×œ× ××ª ×–×”?"
Chitta responds conversationally while guidelines still open
```

### 2. Knowledge Accumulates Continuously

**Principle**: Every conversation turn adds to what we know about the family.

**Implications**:
- No "interview complete" checkpoint
- No minimum required before moving forward
- Quality of knowledge matters more than quantity
- Knowledge informs **prerequisite checking**, not stage gates

**Example**:
```python
# Knowledge state at any moment
knowledge = {
    "child_name": "×“× ×™××œ",
    "age": 3.5,
    "primary_concerns": ["speech"],
    "concern_details": "××•××¨ ×¨×§ ××™×œ×™× ×‘×•×“×“×•×ª, ×œ× ××©×¤×˜×™×. ×”×ª×—×™×œ ×œ×“×‘×¨ ×××•×—×¨.",
    "strengths": "××•×”×‘ ×œ×‘× ×•×ª, ××©×—×§ ×™×¤×” ×œ×‘×“",
    "developmental_history": "",
    "family_context": "",
}

# Enough for guidelines? Check prerequisites
can_generate_guidelines = (
    knowledge.child_name and
    knowledge.age and
    len(knowledge.primary_concerns) > 0 and
    len(knowledge.concern_details) > 100
)
# â†’ True! Offer to generate
```

### 3. Prerequisites Enable, Don't Gate

**Principle**: Prerequisites determine what's **recommended** or **ready**, not what's **blocked**.

**Implications**:
- Parent can ask about anything anytime
- Chitta responds contextually based on readiness
- "Not ready yet" â†’ gentle guidance back, not hard block
- Prerequisites are **qualitative** (enough knowledge?) not **quantitative** (76%?)

**Example**:
```
Parent: "×¨×•×¦×” ×œ×¨××•×ª ×“×•×—"

# Check prerequisites
has_videos = uploaded_videos_count > 0
has_analysis = video_analysis_complete

if has_analysis and report_exists:
    Chitta: "×‘×˜×—! ×”×“×•×— ××•×›×Ÿ. ×¤×•×ª×—×ª ×œ×š ×¢×›×©×™×•"

elif has_videos:
    Chitta: "×”×¡×¨×˜×•× ×™× ×©×œ×š ×‘× ×™×ª×•×—. ×”×“×•×— ×™×”×™×” ××•×›×Ÿ ×‘×§×¨×•×‘!"

elif can_generate_guidelines:
    Chitta: "×× ×™ ×™×›×•×œ×” ×œ×”×›×™×Ÿ ×œ×š ×§×•×“× ×”× ×—×™×•×ª ×œ×¦×™×œ×•×, ×•××– × ×¢×œ×” ×¡×¨×˜×•× ×™×. ×‘×¡×“×¨?"

else:
    Chitta: "×‘×•××™ × ×©×•×—×— ×¢×•×“ ×§×¦×ª ×›×“×™ ×©××•×›×œ ×œ×”×›×™×Ÿ ×œ×š ×”× ×—×™×•×ª ××•×ª×××•×ª"
```

**Note**: No harsh blocks, just **conversational redirection** based on what's ready.

### 4. Capabilities Emerge Gradually

**Principle**: New actions become available as prerequisites are met, not at stage transitions.

**Implications**:
- Cards appear/disappear based on **dependency rules**, not phase
- Actions unlock organically during conversation
- Parent discovers capabilities naturally
- No sudden "you're now in upload phase" transitions

**Example**:
```yaml
# Dependency rules for cards
guidelines_offer_card:
  shows_when:
    - video_guidelines.prerequisites_met: true
    - video_guidelines.exists: false
    - conversation_depth: sufficient

guidelines_ready_card:
  shows_when:
    - video_guidelines.exists: true
    - user_viewed: false

upload_available_card:
  shows_when:
    - video_guidelines.exists: true
    - user_viewed_guidelines: true
    - uploaded_videos_count: 0
```

**User sees**: Cards naturally appear as they talk, creating a sense of **emerging progress** rather than **locked gates**.

### 5. Chitta Guides, Doesn't Control

**Principle**: Chitta **steers** conversation toward needed information but respects parent agency.

**Implications**:
- Proactive suggestions, not demands
- Parent can ignore suggestions and explore
- Chitta adapts to parent's path
- No "you must do X" - only "would you like to do X?"

**Example**:
```
# Chitta has enough for guidelines but parent keeps talking

Chitta detects: Prerequisites met for video_guidelines

# Chitta doesn't STOP conversation or FORCE transition
# Instead, naturally offers when appropriate:

Parent: "×”×•× ×’× ×œ× ×××© ××•×”×‘ ×œ×©×ª×£ ×¦×¢×¦×•×¢×™×"
Chitta: "××‘×™× ×”. ×–×” ××ª×§×©×¨ ×œ××” ×©×©×™×ª×¤×ª ×¢×œ ×”×—×‘×¨×ª×™.

        ×“×¨×š ××’×‘, ×™×© ×œ×™ ×›×‘×¨ ××¡×¤×™×§ ××™×“×¢ ×›×“×™ ×œ×”×›×™×Ÿ ×œ×š ×”× ×—×™×•×ª ×¦×™×œ×•× ××•×ª×××•×ª.
        ×¨×•×¦×” ×©××›×™×Ÿ? ××• ×©×™×© ×¢×•×“ ××©×”×• ×©×—×©×•×‘ ×œ×š ×œ×©×ª×£?"

# Parent chooses:
Option A: "×›×Ÿ, ×ª×›×™× ×™" â†’ Generate
Option B: "×¢×•×“ ×¨×’×¢, ×™×© ×œ×™ ×¢×•×“ ××©×”×•" â†’ Continue conversation
Option C: "××” ×–×” ×”× ×—×™×•×ª?" â†’ Explain, then offer again
```

### 6. Radical Simplicity in UX

**Principle**: User should never think "what do I do now?" - it should be obvious.

**Implications**:
- One clear primary action per state
- Minimal visual complexity
- Conversational prompts instead of UI instructions
- Actions reveal themselves naturally

**Example**:
```
State: Guidelines ready, not viewed yet

What user sees:
1. Conversation ongoing (can still chat)
2. ONE prominent card: "×”×”× ×—×™×•×ª ××•×›× ×•×ª! ğŸ¬" [×œ×—×¦×™ ×œ×¦×¤×™×™×”]
3. Chitta's message: "×”×›× ×ª×™ ×œ×š ×”× ×—×™×•×ª! ×œ×—×¦×™ ×¢×œ ×”×›×¨×˜×™×¡ ×œ××˜×”"

No confusion about "what next?" - it's obvious: click the card.
```

### 7. Proactive Surfacing (Parents Don't Know What Exists)

**Principle**: Parents don't know the internal structure of the app - they cannot ask for everything.

**THE CRITICAL INSIGHT:**

A parent using Chitta for the first time:
- âŒ Doesn't know video guidelines exist
- âŒ Doesn't know they should upload videos
- âŒ Doesn't know a report will be generated
- âŒ Doesn't know what questions they can ask
- âŒ Doesn't know what actions are available

**We cannot rely on parents asking for everything.**

**Solution - Two Information Channels:**

#### Channel 1: Conversation Window (What Chitta Says)

Chitta **proactively offers** capabilities when ready:

```
# Prerequisites met for guidelines
# DON'T wait for parent to ask "can you make guidelines?"
# DO proactively offer:

Chitta: "×™×© ×œ×™ ××¡×¤×™×§ ××™×“×¢ ×›×“×™ ×œ×”×›×™×Ÿ ×œ×š ×”× ×—×™×•×ª ×¦×™×œ×•× ××•×ª×××•×ª
         ×‘××™×•×—×“ ×œ×“× ×™××œ. ×¨×•×¦×” ×©××›×™×Ÿ?"
```

#### Channel 2: Context Cards (Visual Actions)

Cards appear **automatically** when actions are available:

```yaml
guidelines_ready_card:
  appears_automatically: true  # Parent doesn't search for it
  content:
    title: "×”×”× ×—×™×•×ª ××•×›× ×•×ª! ğŸ¬"
    body: "×”× ×—×™×•×ª ×¦×™×œ×•× ××•×ª×××•×ª ×œ×“× ×™××œ"
    action: "×œ×—×¦×™ ×œ×¦×¤×™×™×”"  # Clear what to do
```

**Together**: Parent **always knows** what's happening and what they can do.

#### The "Three Questions" Test

At any moment, parent should be able to answer these **without asking**:

1. **"What's happening right now?"**
   - Answer in: Chitta's latest message OR card title
   - Example: "××›×™× ×” ×¢×‘×•×¨×š..." OR "×”×›× ×ª×™ ×œ×š ×”× ×—×™×•×ª!"

2. **"What can I do now?"**
   - Answer in: Card action button OR Chitta's prompt
   - Example: [×œ×—×¦×™ ×œ×¦×¤×™×™×”] OR "×¨×•×¦×” ×©××›×™×Ÿ?"

3. **"What's next in the process?"**
   - Answer in: Chitta's guidance OR next card appearing
   - Example: "××—×¨×™ ×©×ª×§×¨××™ ××ª ×”×”× ×—×™×•×ª, × ×•×›×œ ×œ×”×¢×œ×•×ª ×¡×¨×˜×•× ×™×"

**If parent can't answer these â†’ UX failure.**

#### Proactive Card Sequencing

Cards appear **in logical sequence**, creating **breadcrumbs**:

```
1. Card appears: "××•×›× /×” ×œ×”× ×—×™×•×ª ×¦×™×œ×•×? ğŸ¬"
   â†“ Parent clicks "×›×Ÿ"

2. Card changes: "××›×™× ×” ×¢×‘×•×¨×š... â³"
   â†“ Wait (5 seconds)

3. Card changes: "×”×”× ×—×™×•×ª ××•×›× ×•×ª! ğŸ¬"
   â†“ Parent clicks "×œ×—×¦×™ ×œ×¦×¤×™×™×”"

4. Opens: Guidelines deep view
   â†“ Parent reads, closes

5. New card: "××•×›× /×” ×œ×”×¢×œ×•×ª ×¡×¨×˜×•× ×™×? ğŸ“¹"
   â†“ Next step is clear
```

**Parent never wonders "what now?"** - each step leads naturally to the next.

#### Contextual Relevance

**Only show what's relevant to current context:**

```python
def get_visible_cards(state: FamilyState) -> List[Card]:
    """Show ONLY cards relevant to current state"""

    visible = []

    # Has guidelines but not viewed? Show that!
    if state.artifacts["video_guidelines"].exists and not state.user_actions["viewed_guidelines"]:
        visible.append(guidelines_ready_card)
        # DON'T show upload card yet - not relevant

    # Viewed guidelines? NOW show upload
    elif state.user_actions["viewed_guidelines"] and state.uploaded_videos_count == 0:
        visible.append(upload_video_card)
        # DON'T show guidelines again - already viewed

    return visible
```

**Result**: Parent sees **1-2 cards maximum**, focused on **what matters now**.

---

## Technical Architecture

### State Model

**OLD (phase-based)**:
```python
class InterviewState:
    phase: Literal["screening", "preparing", "guidelines_ready", "uploading"]
    extracted_data: ExtractedData
    completeness: float  # 0.0 to 1.0
```

**NEW (dependency-based)**:
```python
class FamilyState:
    # Conversation is always active
    conversation_active: bool = True

    # Knowledge accumulates
    extracted_data: ExtractedData
    conversation_history: List[Message]

    # Artifacts that can be generated
    artifacts: Dict[str, Artifact] = {
        "interview_summary": Artifact(exists=False, status=None),
        "video_guidelines": Artifact(exists=False, status=None),
        "assessment_report": Artifact(exists=False, status=None),
    }

    # User actions tracked
    user_actions: Dict[str, bool] = {
        "viewed_guidelines": False,
        "uploaded_first_video": False,
        "viewed_first_report": False,
    }

    # Metadata
    created_at: datetime
    last_active: datetime

class Artifact:
    exists: bool
    status: Optional[Literal["generating", "ready"]]
    content: Optional[str]
    generated_at: Optional[datetime]
    prerequisites_met: bool  # Calculated dynamically
```

### v3.0: Unified Moments Structure (×¤×©×•×˜ - × ×˜×•×œ ×—×œ×§×™× ×¢×•×“×¤×™×)

**Configuration file**: `backend/config/workflows/lifecycle_events.yaml`

#### The Simplification Principle

Wu Wei v3.0 eliminates redundancy by merging three sections into one:

**Moments** = When + What + Message + UI + Unlocks

```yaml
moments:
  guidelines_ready:
    # WHEN does this happen? (Prerequisites)
    when:
      knowledge_is_rich: true

    # WHAT artifact gets generated? (Optional)
    artifact: "baseline_video_guidelines"

    # WHAT message does Chitta send? (Optional)
    message: "×”×”× ×—×™×•×ª ××•×›× ×•×ª! ğŸ“¹"

    # WHAT UI guidance? (Optional, platform-aware)
    ui:
      type: "card"  # card, button, modal, banner, etc.
      default: "×ª×¨××™ ××ª ×”×›×¨×˜×™×¡ '×”× ×—×™×•×ª ×¦×™×œ×•×' ×‘'×¤×¢×™×œ ×¢×›×©×™×•' ×œ××˜×”"
      mobile: "×œ×—×¦×™ ×¢×œ '×”× ×—×™×•×ª' ×‘×ª×¤×¨×™×˜ ×”×ª×—×ª×•×Ÿ"  # Only if different

    # WHAT capabilities unlock? (Optional)
    unlocks:
      - upload_videos
```

#### Always Available Capabilities

```yaml
always_available:
  - conversation      # Talk to Chitta anytime
  - journaling        # Record observations
  - consultation      # Get answers and guidance
```

#### Key Differences from v2.0

| Aspect | v2.0 (Redundant) | v3.0 (Unified) |
|--------|------------------|----------------|
| **Prerequisites** | Defined in both `artifacts` AND `capabilities` | Defined once in `when` |
| **Event mapping** | Artifact has `event:` field linking to separate section | Moment ID IS the event name |
| **Message location** | Separate `lifecycle_events` section | Directly in moment |
| **UI guidance** | Nested `ui_context` with card-specific fields | Flat `ui` with platform fields |
| **Total sections** | 7 (artifacts, capabilities, lifecycle_events, prerequisite_rules, state_indicators, metadata, philosophy) | 3 (always_available, moments, metadata) |

### v3.5: Event-Driven Card Architecture

**Two Types of Cards**:

1. **Event-Triggered Cards** (in `lifecycle_events.yaml`)
   - One-time celebrations/transitions
   - Display when moment triggers
   - Live IN the moment definition
   - No separate display_conditions needed

2. **State-Driven Cards** (in `context_cards.yaml`)
   - Persistent indicators/reminders
   - Display while conditions are true
   - Examples: conversation_depth, video_guidelines_reminder

**Example:**

```yaml
# lifecycle_events.yaml
moments:
  guidelines_ready:
    when: { knowledge_is_rich: true }
    artifact: "baseline_video_guidelines"

    # ğŸŒŸ Card appears when moment triggers (event-driven)
    card:
      card_type: success
      priority: 100
      title: "×”×”× ×—×™×•×ª ××•×›× ×•×ª! ğŸ¬"
      body: "×”×›× ×ª×™ ×œ×š ×”× ×—×™×•×ª ×¦×™×œ×•× ××•×ª×××•×ª ×œ{child_name}."
      actions: [view_video_guidelines]
      dismissible: false
```

**Backend Flow:**

```python
# 1. Moment triggers
lifecycle_result = await lifecycle_manager.process_lifecycle_events(...)

# 2. Extract event cards
event_cards = self._extract_event_cards(lifecycle_result)

# 3. Generate state cards
state_cards = card_generator.get_visible_cards(context)

# 4. Merge: Event cards first (higher priority)
context_cards = event_cards + state_cards

# 5. Send to frontend
return {"ui_data": {"cards": context_cards}}
```

**Domain-Agnostic Context:**

Backend passes generic structures, cards resolve fields via placeholders:

```python
# Backend (domain-agnostic)
context = {
    "extracted_data": session.extracted_data,  # Whole object
    "artifacts": session.artifacts,
    "message_count": len(conversation_history),
}

# Card YAML (domain-specific)
title: "×¤×¨×•×¤×™×œ: {child_name}"  # Resolved from extracted_data.child_name
body: "×’×™×œ {age}"               # Resolved from extracted_data.age
```

**Message Timing Fix:**

User messages now saved BEFORE lifecycle checks for accurate message_count:

```python
# 1. Extract data from user message
# 2. ğŸ’¾ Save user message (NEW: moved before lifecycle)
# 3. ğŸ”„ Refresh session (get updated count)
# 4. âœ… Check lifecycle events (sees correct message_count)
# 5. Generate response
# 6. ğŸ’¾ Save assistant response
```
| **Lines of config** | 360 | 208 |

#### Example: Complete Moment

```yaml
moments:
  report_ready:
    when:
      baseline_video_analysis.exists: true
      OR:
        conversation_knowledge_is_rich: true

    artifact: "baseline_parent_report"

    message: |
      ×”×“×•×— ××•×›×Ÿ! ğŸ“„

      ×–×” ×”×™×” ×ª×”×œ×™×š ×¢×©×™×¨ - ×ª×•×“×” ×©×”×©×§×¢×ª ××ª ×”×–××Ÿ ×œ×©×ª×£ ×•×œ×¦×œ×.
      ××¢×›×©×™×• ×× ×™ ×›××Ÿ ×‘×©×‘×™×œ×š ×œ×›×œ ×©××œ×”. ğŸ’™

    ui:
      type: "card"
      default: "×œ×—×¦×™ ×¢×œ ×”×›×¨×˜×™×¡ '××“×¨×™×š ×œ×”×•×¨×™×' ×‘'×¤×¢×™×œ ×¢×›×©×™×•' ×œ××˜×”"

    unlocks:
      - view_reports
      - find_experts
      - start_re_assessment
```

#### How It Works

```python
# LifecycleManager simplified in v3.0
moments = config.get("moments", {})

for moment_id, moment_config in moments.items():
    # Check prerequisites from 'when' field
    prerequisites = moment_config.get("when")
    prereqs_met = evaluate_prerequisites(prerequisites, context)

    # If prerequisites just became met (transition)
    if prereqs_met and not previously_met:

        # Generate artifact if defined
        artifact_id = moment_config.get("artifact")
        if artifact_id:
            generate_artifact(artifact_id, moment_config, context)

        # Send message if defined
        message = moment_config.get("message")
        if message:
            send_message(message.format(child_name=child_name))

        # Include UI guidance if defined
        ui_context = moment_config.get("ui")
        if ui_context:
            include_ui_guidance(ui_context, platform)

        # Unlock capabilities if defined
        unlocks = moment_config.get("unlocks", [])
        unlock_capabilities(unlocks)
```

**Benefits**:
- âœ… Everything about a moment in ONE place
- âœ… No redundant prerequisite definitions
- âœ… No separate event mapping needed
- âœ… Flatter, simpler structure
- âœ… Easy to understand and modify
- âœ… 50% less configuration

### Prerequisite System

**Core function**:
```python
def check_prerequisites(
    capability: str,
    state: FamilyState
) -> PrerequisiteCheck:
    """
    Check if prerequisites are met for a capability.

    Returns:
        PrerequisiteCheck with:
            - met: bool (are prerequisites satisfied?)
            - missing: List[str] (what's missing?)
            - readiness: Literal["ready", "need_more", "optional"]
            - suggestion: str (what to tell parent)
    """

    if capability == "video_guidelines":
        has_basic = state.extracted_data.child_name and state.extracted_data.age
        has_concerns = len(state.extracted_data.primary_concerns) > 0
        has_details = len(state.extracted_data.concern_details or "") > 100

        if has_basic and has_concerns and has_details:
            return PrerequisiteCheck(
                met=True,
                readiness="ready",
                suggestion="×™×© ×œ×™ ××¡×¤×™×§ ××™×“×¢ ×œ×”×›×™×Ÿ ×”× ×—×™×•×ª. ×¨×•×¦×”?"
            )
        else:
            missing = []
            if not has_basic:
                missing.append("×©× ×•×’×™×œ")
            if not has_concerns:
                missing.append("×ª×—×•××™ ×“××’×”")
            if not has_details:
                missing.append("×¢×•×“ ×¤×¨×˜×™× ×¢×œ ×”×“××’×•×ª")

            return PrerequisiteCheck(
                met=False,
                missing=missing,
                readiness="need_more",
                suggestion=f"×‘×•××™ × ×©×•×—×— ×¢×•×“ ×§×¦×ª ×¢×œ: {', '.join(missing)}"
            )

    elif capability == "video_upload":
        # Can ALWAYS upload, but better with guidelines
        has_guidelines = state.artifacts["video_guidelines"].exists

        return PrerequisiteCheck(
            met=True,  # Never blocked
            readiness="optional" if not has_guidelines else "ready",
            suggestion=(
                "××¤×©×¨ ×œ×”×¢×œ×•×ª ×¢×›×©×™×•, ××‘×œ ×× ×™ ×××œ×™×¦×” ×§×•×“× ×œ×§×¨×•× ××ª ×”×”× ×—×™×•×ª"
                if not has_guidelines else
                "××•×›× /×” ×œ×”×¢×œ×•×ª ×¡×¨×˜×•× ×™×?"
            )
        )
```

### Artifact Generation

**Triggered by user intent + prerequisites**:

```python
async def handle_user_intent(
    family_id: str,
    intent: Intent,
    state: FamilyState
):
    """
    Handle user expressing intent to do something.

    User can express intent via:
    - Direct question: "××™×š ××¢×œ×™× ×¡×¨×˜×•×Ÿ?"
    - Direct request: "×ª×›×™× ×™ ×œ×™ ×”× ×—×™×•×ª"
    - Function call: check_interview_completeness
    - Chitta suggestion: "×¨×•×¦×” ×©××›×™×Ÿ ×”× ×—×™×•×ª?"
    """

    if intent.type == "generate_guidelines":
        # Check prerequisites
        prereq_check = check_prerequisites("video_guidelines", state)

        if prereq_check.met:
            # Start generation
            state.artifacts["video_guidelines"].status = "generating"

            # Show loading card
            await send_card_update(family_id, "preparing_guidelines_card")

            # Generate with strong model
            guidelines = await generate_video_guidelines_artifact(
                family_id=family_id,
                model="gemini-2.0-flash-exp",
                extracted_data=state.extracted_data
            )

            # Store artifact
            state.artifacts["video_guidelines"] = Artifact(
                exists=True,
                status="ready",
                content=guidelines,
                generated_at=datetime.now()
            )

            # Notify user
            await send_message(
                family_id,
                "×”×›× ×ª×™ ×œ×š! ğŸ‰ ×œ×—×¦×™ ×¢×œ ×”×›×¨×˜×™×¡ ×œ××˜×” ×›×“×™ ×œ×¨××•×ª ××ª ×”×”× ×—×™×•×ª"
            )

            # Show ready card
            await send_card_update(family_id, "guidelines_ready_card")

        else:
            # Prerequisites not met - guide conversationally
            await send_message(
                family_id,
                prereq_check.suggestion
            )
```

### Qualitative Progress Indicators

**Replace percentage with conversational hints:**

```python
def get_knowledge_depth_indicator(data: ExtractedData) -> dict:
    """Get qualitative indicator of conversation depth"""

    if not data.child_name:
        return {
            "emoji": "ğŸ‘‹",
            "text": "×”×ª×—×œ× ×• ×œ×”×›×™×¨",
            "level": "minimal"
        }

    if not data.primary_concerns or len(data.concern_details or "") < 100:
        return {
            "emoji": "ğŸ’­",
            "text": f"××›×™×¨×™× ××ª {data.child_name}...",
            "level": "growing"
        }

    if len(data.concern_details or "") < 300:
        return {
            "emoji": "ğŸ’­",
            "text": "×”×©×™×—×” ××ª×¢××§×ª",
            "level": "developing"
        }

    return {
        "emoji": "ğŸ’™",
        "text": f"×”×›×¨× ×• ××ª {data.child_name}",
        "level": "rich"
    }
```

### Card System (Dependency-Based)

**Configuration** (`context_cards.yaml`):

```yaml
cards:
  conversation_depth_hint:
    name: "×¨××– ×¢×œ ×¢×•××§ ×”×©×™×—×”"
    card_type: info
    priority: 30

    display_conditions:
      conversation_active: true

    content:
      title: "{knowledge_depth_indicator}"
      body: "×ª×•×“×” ×¢×œ ×”×©×™×ª×•×£ ğŸ’™"

    dismissible: true

  guidelines_offer:
    name: "×”×¦×¢×” ×œ×”×›× ×ª ×”× ×—×™×•×ª"
    card_type: suggestion
    priority: 80

    display_conditions:
      artifacts.video_guidelines.prerequisites_met: true
      artifacts.video_guidelines.exists: false
      user_declined_offer: false

    content:
      title: "××•×›× /×” ×œ×”× ×—×™×•×ª ×¦×™×œ×•×? ğŸ¬"
      body: "×™×© ×œ×™ ××¡×¤×™×§ ××™×“×¢ ×›×“×™ ×œ×”×›×™×Ÿ ×œ×š ×”× ×—×™×•×ª ××•×ª×××•×ª"

    actions:
      - name: "×›×Ÿ, ×ª×›×™× ×™"
        triggers: generate_guidelines
      - name: "×¢×•×“ ×¨×’×¢"
        dismisses_card: true

  guidelines_preparing:
    name: "××›×™× ×” ×”× ×—×™×•×ª"
    card_type: loading
    priority: 100

    display_conditions:
      artifacts.video_guidelines.status: "generating"

    content:
      title: "××›×™× ×” ×¢×‘×•×¨×š... â³"
      body: |
        âœ¨ ×”× ×—×™×•×ª ×¦×™×œ×•× ××•×ª×××•×ª
        ×¨×’×¢ ×§×˜×Ÿ...

    dismissible: false
    auto_replaces_with: "guidelines_ready"

  guidelines_ready:
    name: "×”× ×—×™×•×ª ××•×›× ×•×ª"
    card_type: success
    priority: 100

    display_conditions:
      artifacts.video_guidelines.exists: true
      user_actions.viewed_guidelines: false

    content:
      title: "×”×”× ×—×™×•×ª ××•×›× ×•×ª! ğŸ¬"
      body: "×”× ×—×™×•×ª ×¦×™×œ×•× ××•×ª×××•×ª ×œ{child_name}"

    actions:
      - name: "×œ×—×¦×™ ×œ×¦×¤×™×™×”"
        opens_view: "guidelines_deep_view"
        tracks_action: "viewed_guidelines"

  upload_available:
    name: "××¤×©×¨ ×œ×”×¢×œ×•×ª"
    card_type: primary
    priority: 90

    display_conditions:
      artifacts.video_guidelines.exists: true
      user_actions.viewed_guidelines: true
      uploaded_videos_count: 0

    content:
      title: "××•×›× /×” ×œ×”×¢×œ×•×ª? ğŸ“¹"
      body: "×§×¨××ª ××ª ×”×”× ×—×™×•×ª - ××¤×©×¨ ×œ×”×ª×—×™×œ"

    actions:
      - name: "×”×¢×œ×™ ×¡×¨×˜×•× ×™×"
        opens_view: "video_upload_view"
```

### Proactive Suggestion System

```python
async def suggest_next_capability(
    family_id: str,
    state: FamilyState
) -> Optional[str]:
    """
    Proactively suggest next capability when prerequisites are met.

    This ensures parent doesn't have to ask - Chitta offers.
    """

    # Check what's ready but not yet offered
    if not state.artifacts["video_guidelines"].exists:
        prereq_check = check_prerequisites("video_guidelines", state)
        if prereq_check.met and not state.user_actions.get("offered_guidelines"):
            state.user_actions["offered_guidelines"] = True
            return "generate_guidelines"

    elif state.artifacts["video_guidelines"].exists and not state.user_actions["viewed_guidelines"]:
        return "view_guidelines"

    elif state.user_actions["viewed_guidelines"] and state.uploaded_videos_count == 0:
        if not state.user_actions.get("offered_upload"):
            state.user_actions["offered_upload"] = True
            return "upload_video"

    elif state.uploaded_videos_count > 0 and state.analysis_status == "complete":
        if not state.user_actions.get("offered_report"):
            state.user_actions["offered_report"] = True
            return "view_report"

    return None


def inject_proactive_suggestion(
    base_response: str,
    suggestion: Optional[str],
    state: FamilyState
) -> str:
    """
    Inject proactive suggestion into Chitta's response.
    Makes capabilities discoverable without parent asking.
    """

    if not suggestion:
        return base_response

    suggestions = {
        "generate_guidelines": f"\n\n×“×¨×š ××’×‘ - ×™×© ×œ×™ ××¡×¤×™×§ ××™×“×¢ ×›×“×™ ×œ×”×›×™×Ÿ ×œ×š ×”× ×—×™×•×ª ×¦×™×œ×•× ××•×ª×××•×ª ×œ{state.extracted_data.child_name}. ×¨×•×¦×” ×©××›×™×Ÿ?",

        "view_guidelines": f"\n\n×”×›× ×ª×™ ×œ×š ×”× ×—×™×•×ª ×¦×™×œ×•×! ×œ×—×¦×™ ×¢×œ ×”×›×¨×˜×™×¡ ×œ××˜×” ×›×“×™ ×œ×§×¨×•×",

        "upload_video": f"\n\n×§×¨××ª ××ª ×”×”× ×—×™×•×ª? ××•×›× /×” ×œ×”×¢×œ×•×ª ×¡×¨×˜×•× ×™×?",

        "view_report": f"\n\n×”×“×•×— ××•×›×Ÿ! ×¨×•×¦×” ×œ×¨××•×ª ××•×ª×•?"
    }

    return base_response + suggestions.get(suggestion, "")
```

---

## User Experience Journey

### Journey 1: First-Time Parent (Natural Flow)

**Parent opens Chitta, has NEVER used it before:**

```
1. Chitta: "×©×œ×•×! ×× ×™ ×¦'×™×˜×”. ×× ×™ ×›××Ÿ ×œ×¢×–×•×¨ ×œ×š ×œ×”×‘×™×Ÿ ××ª ×”×”×ª×¤×ª×—×•×ª
            ×©×œ ×”×™×œ×“/×” ×©×œ×š. ×‘×•××™ × ×ª×—×™×œ - ×¡×¤×¨×™ ×œ×™ ×¢×œ ×”×™×œ×“/×”"

   What parent knows:
   âœ… What Chitta is
   âœ… What to do next (×¡×¤×¨×™ ×œ×™)
   âœ… How to proceed (input box visible)

2. Parent: "×™×© ×œ×™ ×‘×Ÿ ×‘×Ÿ 3, ×“× ×™××œ"
   Chitta: "× ×¢×™× ×œ×”×›×™×¨ ××ª ×“× ×™××œ! ××” ××¢×¡×™×§ ××•×ª×š?"

3. Parent: "×”×•× ×œ× ×××© ××“×‘×¨ ×˜×•×‘"
   Chitta: "×¡×¤×¨×™ ×œ×™ ×¢×•×“ - ××™×š ×–×” ×‘× ×œ×™×“×™ ×‘×™×˜×•×™?"

   [Card appears: "×”×©×™×—×” ××ª×¢××§×ª ğŸ’­"]

4. Parent: "×”×•× ××•××¨ ×¨×§ ××™×œ×™× ×‘×•×“×“×•×ª"
   Chitta: "××‘×™× ×”. ×¢×•×“ ××©×”×• ×—×©×•×‘?"

5. Parent: "×”×•× ××•×”×‘ ×œ×‘× ×•×ª, ××©×—×§ ×™×¤×” ×œ×‘×“"
   Chitta: "× ×”×“×¨! ×™×© ×¢×•×“ ××©×”×•?"

6. Parent: "×–×” ×‘×¢×¦× ×”×›×œ"

   [Prerequisites met! âœ“]

   Chitta: "×ª×•×“×” ×¢×œ ×”×©×™×ª×•×£! ×™×© ×œ×™ ××¡×¤×™×§ ××™×“×¢ ×›×“×™ ×œ×”×›×™×Ÿ ×œ×š
           ×”× ×—×™×•×ª ×¦×™×œ×•× ××•×ª×××•×ª ×œ×“× ×™××œ. ×¨×•×¦×” ×©××›×™×Ÿ?"

   [Card appears: "××•×›× /×” ×œ×”× ×—×™×•×ª ×¦×™×œ×•×? ğŸ¬" [×›×Ÿ]]

   What parent knows:
   âœ… Chitta CAN create guidelines (didn't know this existed!)
   âœ… What guidelines are (×”× ×—×™×•×ª ×¦×™×œ×•×)
   âœ… They're personalized (×œ×“× ×™××œ)
   âœ… How to proceed (click ×›×Ÿ)
   âœ… It's optional (×¨×•×¦×”? not "you must")

7. Parent clicks "×›×Ÿ"

   Chitta: "××›×™× ×”... ×¨×’×¢ ×§×˜×Ÿ"
   [Card: "××›×™× ×” ×¢×‘×•×¨×š... â³"]

   What parent knows:
   âœ… Something is being prepared
   âœ… It will take a moment
   âœ… No action needed

8. [5 seconds later]

   Chitta: "××•×›×Ÿ! ×”×›× ×ª×™ ×œ×š ×”× ×—×™×•×ª ×©××ª××§×“×•×ª ×‘×“×™×‘×•×¨ ×•×ª×§×©×•×¨×ª ×©×œ ×“× ×™××œ.
           ×œ×—×¦×™ ×¢×œ ×”×›×¨×˜×™×¡ ×œ××˜×” ×›×“×™ ×œ×§×¨×•×"

   [Card: "×”×”× ×—×™×•×ª ××•×›× ×•×ª! ğŸ¬" [×œ×—×¦×™ ×œ×¦×¤×™×™×”]]

   What parent knows:
   âœ… Guidelines ready
   âœ… What they focus on (×“×™×‘×•×¨ ×•×ª×§×©×•×¨×ª)
   âœ… How to view (×œ×—×¦×™ ×¢×œ ×”×›×¨×˜×™×¡)

9. Parent clicks â†’ Guidelines open

10. Parent reads, closes view

    [New card: "××•×›× /×” ×œ×”×¢×œ×•×ª? ğŸ“¹"]

    Chitta: "×§×¨××ª ××ª ×”×”× ×—×™×•×ª? ××•×›× /×” ×œ×”×¢×œ×•×ª ×¡×¨×˜×•× ×™×?"
```

**Key points:**
- âœ… Parent NEVER had to ask what's possible
- âœ… Each step was surfaced proactively
- âœ… Clear what to do at every moment
- âœ… Natural conversation flow maintained

### Journey 2: Curious Parent (Explores Before Ready)

```
1. Parent: "××™×š ××¢×œ×™× ×¡×¨×˜×•×Ÿ?"  â† Jumps ahead!

2. Chitta checks prerequisites:
   - has_guidelines? No
   - has_knowledge? No
   - can_upload? Yes (not blocked!)

3. Chitta: "×©××œ×” ×˜×•×‘×”! ××¤×©×¨ ×œ×”×¢×œ×•×ª ×¡×¨×˜×•× ×™× ×‘×›×œ ×¨×’×¢.

           ××‘×œ ×× ×™ ×××œ×™×¦×” ×§×•×“× ×©× ×›×™×¨ ×§×¦×ª - ×–×” ×™×¢×–×•×¨ ×œ×™ ×œ×”×›×™×Ÿ
           ×œ×š ×”× ×—×™×•×ª ×××© ××•×ª×××•×ª ×œ×¦×¨×›×™× ×©×œ ×”×™×œ×“/×” ×©×œ×š.

           ×¨×•×¦×” ×©× ×ª×—×™×œ? ×¡×¤×¨×™ ×œ×™ ×¢×œ ×”×™×œ×“/×”"

   What parent knows:
   âœ… Can upload anytime (not blocked)
   âœ… Better to talk first (gentle guidance)
   âœ… Why it's better (×”× ×—×™×•×ª ××•×ª×××•×ª)
   âœ… What to do (×¡×¤×¨×™ ×œ×™)

4. Parent: "××•×§×™×™. ×™×© ×œ×™ ×‘×ª..."
   [Conversation continues]
```

**Key point**: Question answered honestly, gently guided back. No harsh block.

### Journey 3: Rushed Parent

```
1. Parent: "×¨×•×¦×” ×¤×©×•×˜ ×œ×”×¢×œ×•×ª ×¡×¨×˜×•×Ÿ"

2. Chitta: "×‘×¡×“×¨ ×’××•×¨! ×× ×™ ×™×›×•×œ×” ×œ×¤×ª×•×— ×œ×š ××ª ×”×¢×œ××ª ×”×¡×¨×˜×•× ×™× ×¢×›×©×™×•.

           ××• - ×× ×™×© ×œ×š 2 ×“×§×•×ª, ×× ×™ ×™×›×•×œ×” ×œ×”×›×™×Ÿ ×œ×š ×”× ×—×™×•×ª ×§×¦×¨×•×ª
           ×©×™×¢×–×¨×• ×œ×¦×œ× ×‘×¦×•×¨×” ×”×›×™ ××•×¢×™×œ×”. ××” × ×•×— ×œ×š?"

3. Parent: "×‘×•××™ ×ª×›×™× ×™ ××©×”×• ××”×™×¨"
   [Quick 2-minute conversation]

4. Chitta: "××¡×¤×™×§! ×× ×™ ××›×™× ×” ×œ×š ×”× ×—×™×•×ª ×§×¦×¨×•×ª"
   [Generates concise guidelines]

5. Parent uploads without reading
   â†’ Works! Analysis proceeds

6. Later: "××” ×”× ×—×™×¤×©×•?"
   Chitta: "×™×© ×œ×š ×”× ×—×™×•×ª ×©×”×›× ×ª×™ - ×¨×•×¦×” ×œ×¨××•×ª?"
```

**Key point**: Urgency respected, value offered, never forced.

---

## Implementation Guide

### Phase 1: Remove Phase System

**Files to modify:**

1. **`backend/app/services/interview_service.py`**
```python
# REMOVE:
class InterviewState:
    phase: str = "screening"

# ADD:
class InterviewState:
    conversation_active: bool = True
    artifacts: Dict[str, Artifact] = field(default_factory=dict)
    user_actions: Dict[str, bool] = field(default_factory=dict)
```

2. **`backend/app/services/conversation_service.py`**
```python
# REMOVE all phase checks:
if session.phase == "screening":
    ...

# REPLACE with prerequisite checks:
prereq_check = check_prerequisites("video_guidelines", state)
if prereq_check.met:
    ...
```

### Phase 2: Add Prerequisite System

Update **`backend/app/services/prerequisite_service.py`**:

```python
from typing import Dict, List, Literal
from dataclasses import dataclass

@dataclass
class PrerequisiteCheck:
    met: bool
    missing: List[str]
    readiness: Literal["ready", "need_more", "optional"]
    suggestion: str

class PrerequisiteService:
    def check_video_guidelines(self, state: FamilyState) -> PrerequisiteCheck:
        has_basic = state.extracted_data.child_name and state.extracted_data.age
        has_concerns = len(state.extracted_data.primary_concerns) > 0
        has_details = len(state.extracted_data.concern_details or "") > 100

        if has_basic and has_concerns and has_details:
            return PrerequisiteCheck(
                met=True,
                missing=[],
                readiness="ready",
                suggestion=f"×™×© ×œ×™ ××¡×¤×™×§ ××™×“×¢ ×œ×”×›×™×Ÿ ×”× ×—×™×•×ª. ×¨×•×¦×”?"
            )
        # ... handle not ready case
```

### Phase 3: Add Artifact System

Create **`backend/app/services/artifact_service.py`**:

```python
class ArtifactService:
    async def generate_video_guidelines(
        self,
        family_id: str,
        extracted_data: ExtractedData,
        model: str = "gemini-2.0-flash-exp"
    ) -> str:
        # Set status
        self.set_artifact_status(family_id, "video_guidelines", "generating")

        # Build prompt
        prompt = self._build_guidelines_prompt(extracted_data)

        # Use strong model
        strong_llm = create_llm_provider(model=model)
        result = await strong_llm.chat(
            messages=[Message(role="user", content=prompt)],
            temperature=0.5,
            max_tokens=3000
        )

        # Store
        self.store_artifact(family_id, "video_guidelines", result.content)

        return result.content
```

### Phase 4: Add Qualitative Progress

Create **`backend/app/services/knowledge_indicator_service.py`**:

```python
def get_knowledge_depth_indicator(data: ExtractedData) -> dict:
    if not data.child_name:
        return {"emoji": "ğŸ‘‹", "text": "×”×ª×—×œ× ×• ×œ×”×›×™×¨"}

    if len(data.concern_details or "") < 100:
        return {"emoji": "ğŸ’­", "text": f"××›×™×¨×™× ××ª {data.child_name}..."}

    if len(data.concern_details or "") < 300:
        return {"emoji": "ğŸ’­", "text": "×”×©×™×—×” ××ª×¢××§×ª"}

    return {"emoji": "ğŸ’™", "text": f"×”×›×¨× ×• ××ª {data.child_name}"}
```

### Phase 5: Update Card System

Modify **`backend/config/workflows/context_cards.yaml`**:

Replace phase-based conditions:
```yaml
# OLD:
display_conditions:
  phase: guidelines_ready

# NEW:
display_conditions:
  artifacts.video_guidelines.exists: true
  user_actions.viewed_guidelines: false
```

### Phase 6: Add Proactive Suggestions

In **`conversation_service.py`**, add:

```python
# After generating response
suggestion = await suggest_next_capability(family_id, state)
if suggestion:
    response = inject_proactive_suggestion(response, suggestion, state)
```

### Phase 7: Update Frontend

**`src/App.jsx`**:

```javascript
// OLD:
const [completeness, setCompleteness] = useState(0);
<div>×”×©×œ×× ×• {completeness}% ××”×¨××™×•×Ÿ</div>

// NEW:
const [knowledgeIndicator, setKnowledgeIndicator] = useState({
  emoji: "ğŸ‘‹",
  text: "×”×ª×—×œ× ×• ×œ×”×›×™×¨"
});

<div className="text-sm text-gray-500">
  {knowledgeIndicator.emoji} {knowledgeIndicator.text}
</div>
```

---

## Examples & Scenarios

### Complete User Flow Example

```
INITIAL STATE:
- No knowledge collected
- No artifacts exist
- Cards: None

Parent: "×©×œ×•×"
Chitta: "×©×œ×•×! ×× ×™ ×¦'×™×˜×”. ×¡×¤×¨×™ ×œ×™ ×¢×œ ×”×™×œ×“/×” ×©×œ×š"
Cards: [conversation_depth: "×”×ª×—×œ× ×• ×œ×”×›×™×¨ ğŸ‘‹"]

---

AFTER SHARING NAME + AGE:
- Knowledge: name=×“× ×™××œ, age=3.5
- Artifacts: None
- Cards: [conversation_depth: "××›×™×¨×™× ××ª ×“× ×™××œ... ğŸ’­"]

---

AFTER SHARING CONCERNS:
- Knowledge: concerns=[speech], details="××•××¨ ××™×œ×™× ×‘×•×“×“×•×ª"
- Prerequisites MET: video_guidelines
- Chitta: "×“×¨×š ××’×‘ - ×™×© ×œ×™ ××¡×¤×™×§ ××™×“×¢ ×œ×”×›×™×Ÿ ×”× ×—×™×•×ª. ×¨×•×¦×”?"
- Cards: [guidelines_offer: "××•×›× /×” ×œ×”× ×—×™×•×ª? ğŸ¬"]

---

PARENT CLICKS "×›×Ÿ":
- Artifact status: video_guidelines = "generating"
- Chitta: "××›×™× ×”... ×¨×’×¢ ×§×˜×Ÿ"
- Cards: [guidelines_preparing: "××›×™× ×” ×¢×‘×•×¨×š... â³"]

---

5 SECONDS LATER:
- Artifact: video_guidelines = "ready"
- Chitta: "××•×›×Ÿ! ×œ×—×¦×™ ×¢×œ ×”×›×¨×˜×™×¡"
- Cards: [guidelines_ready: "×”×”× ×—×™×•×ª ××•×›× ×•×ª! ğŸ¬"]

---

PARENT CLICKS CARD:
- Opens: Guidelines deep view
- User action: viewed_guidelines = true
- Conversation continues in background

---

PARENT CLOSES VIEW:
- Cards: [upload_available: "××•×›× /×” ×œ×”×¢×œ×•×ª? ğŸ“¹"]
- Chitta: "×§×¨××ª ××ª ×”×”× ×—×™×•×ª? ××•×›× /×” ×œ×”×¢×œ×•×ª?"

---

And so on... Flow continues naturally based on dependencies!
```

---

## Summary: Wu Wei Principles

**What changes:**
- âœ… Remove rigid phases â†’ Dependency graph
- âœ… Remove "interview complete" â†’ Continuous conversation
- âœ… Remove percentage â†’ Qualitative hints
- âœ… Add prerequisite checking
- âœ… Add artifact generation (triggered, not forced)
- âœ… Cards based on dependencies
- âœ… Proactive surfacing of capabilities

**What stays true to Wu Wei:**
- ğŸŒŠ Natural flow - no forcing
- ğŸŒŠ Parent has agency - explores freely
- ğŸŒŠ Chitta guides - suggests when ready
- ğŸŒŠ Prerequisites enable - don't gate
- ğŸŒŠ Conversation primary - activities are context
- ğŸŒŠ Proactive surfacing - parent never guesses
- ğŸŒŠ Two channels - conversation + cards
- ğŸŒŠ Clear next steps - always obvious

**Result**: Parents experience Chitta as a **helpful, intelligent guide** that flows naturally with their needs, not a rigid system forcing them through hoops.

---

**Conversation flows like water ğŸŒŠ**
**Knowledge accumulates like spring ğŸ’§**
**Capabilities emerge like flowers ğŸŒ¸**
**Parents explore like wind ğŸƒ**
**Chitta guides like the moon ğŸŒ™**

**Not forced. Not gated. Just... flowing.**

---

**End of Document** ğŸ’™
