# LLM Configuration
# Choose provider: "gemini", "anthropic", "openai", or "simulated"
LLM_PROVIDER=gemini

# Model Selection
LLM_MODEL=gemini-2.0-flash-exp  # Options: gemini-2.0-flash-exp, gemini-pro-2.5, gemini-2.5-pro

# Enhanced Mode (NEW - for improved function calling)
# Set to "true" to enable fallback extraction and monitoring for less capable models
# Recommended: true for Flash models, optional for Pro models
LLM_USE_ENHANCED=true

# Gemini API (recommended for cost efficiency)
GEMINI_API_KEY=your_gemini_api_key_here

# Anthropic API (optional - for Claude)
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# OpenAI API (optional - for GPT-4)
OPENAI_API_KEY=your_openai_api_key_here

# Application
ENVIRONMENT=development
LOG_LEVEL=INFO

# CORS (for frontend)
CORS_ORIGINS=http://localhost:5173,http://localhost:3000
