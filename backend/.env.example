# LLM Configuration
# Choose provider: "gemini", "anthropic", "openai", or "simulated"
LLM_PROVIDER=gemini

# Model Selection
# For conversation: Use fast model for better UX
LLM_MODEL=gemini-flash-lite-latest  # Options: gemini-flash-lite-latest, gemini-2.5-flash, gemini-2.5-pro

# For extraction: Use stronger model for accurate categorization
EXTRACTION_MODEL=gemini-2.5-flash  # Recommended: gemini-2.5-flash (stable function calling)

# For artifact generation (video guidelines, reports): Use strongest model for quality
STRONG_LLM_MODEL=gemini-2.0-flash-exp  # Options: gemini-2.0-flash-exp, gemini-2.5-pro, claude-3-5-sonnet-20241022

# Enhanced Mode (NEW - for improved function calling)
# Set to "true" to enable fallback extraction and monitoring for less capable models
# Recommended: true for Flash models, optional for Pro models
LLM_USE_ENHANCED=true

# Gemini API (recommended for cost efficiency)
GEMINI_API_KEY=your_gemini_api_key_here

# Anthropic API (optional - for Claude)
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# OpenAI API (optional - for GPT-4)
OPENAI_API_KEY=your_openai_api_key_here

# Application
ENVIRONMENT=development
LOG_LEVEL=INFO

# CORS (for frontend)
CORS_ORIGINS=http://localhost:5173,http://localhost:3000
